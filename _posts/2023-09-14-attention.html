---
layout: post
title:  "Modeling 3 - Attention (9/14/2023)"
date:   2023-09-14 13:30:00 -0400
categories: classes
---

<p><b>Content:</b>
<ul class="bullet">
  <li>Attention</li>
  <li>What do We Attend To?</li>
  <li>Improvements to Attention</li>
  <li>Specialized Attention Varieties</li>
  <li>A Case Study: "Attention is All You Need"</li>
</ul>
</p>
<p>
<b>Reading Material</b>
<ul class="default">
  <li><i>Required Reading:</i> <a href="https://arxiv.org/pdf/1703.01619.pdf">Neural Machine Translation and Sequence-to-Sequence Models</a> Chapter 8</li>
  <li><i>Required Reading:</i> <a href="http://nlp.seas.harvard.edu/annotated-transformer/">The Annotated Transformer</a></li>
  <li><i>Reference:</i> <a href="https://transformer-circuits.pub/2021/framework/index.html">A Mathematical Framework for Transformer Circuit</a> (Elhage et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1409.0473.pdf">Attentional NMT</a> (Bahdanau et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D15/D15-1166.pdf">Effective Approaches to Attention</a> (Luong et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1603.06393.pdf">Copying Mechanism</a> (Gu et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1162.pdf">Attention-based Bias</a> (Arthur et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1502.03044.pdf">Attending to Images</a> (Xu et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1508.01211.pdf">Attending to Speech</a> (Chan et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N16-1174.pdf">Hierarchical Attention</a> (Yang et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1601.00710.pdf">Attending to Multiple Sources</a> (Zoph and Knight 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-2031.pdf">Different Multi-source Strategies</a> (Libovicky and Helcl 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/W16-2360.pdf">Multi-modal Attention</a> (Huang et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://www.aclweb.org/old_anthology/D/D16/D16-1053.pdf">Self Attention</a> (Cheng et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is All You Need</a> (Vaswani et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P18-1166.pdf">Slow Transformer Decoding</a> (Zhang et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P18-1008.pdf">Transformer+RNN Hybrid Models</a> (Chen et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1910.05895">Training Transformers on Small Data</a> (Nguyen and Salazar 2019)</li>
  <!-- <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N16-1102.pdf">Structural Biases in Attention</a> (Cohn et al. 2015)</li> -->
  <!-- <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D16-1096.pdf">Coverage Embedding Models</a> (Mi et al. 2016)</li> -->
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1011.pdf">Interpretability w/ Hard Attention</a> (Lei et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1249.pdf">Supervised Attention</a> (Mi et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1706.03872.pdf">Attention vs. Alignment</a> (Koehn and Knowles 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1902.10186">Attention is not Explanation</a> (Jain and Wallace 2019)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.432.pdf">Learning to Deceive w/ Attention</a> (Pruthi et al. 2020)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1138.pdf">Monotonic Attention</a> (Yu et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1602.03001.pdf">Convolutional Attention</a> (Allamanis et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://www.sciencedirect.com/science/article/pii/S0925231218300225">Fine-grained Attention</a> (Choi et al. 2018)</li>
</ul>
</p>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-06-attention.pdf">Attention Slides</a> <br/>
<b>Sample Code:</b> <a href="https://github.com/neubig/nn4nlp-code/tree/master/09-attention">Attention Code Examples</a><br/>
</p>
