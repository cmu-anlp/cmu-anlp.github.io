---
layout: post
title:  "Modeling 2 - Conditioned Generation (9/12/2023)"
date:   2023-09-12 13:30:00 -0400
categories: classes
---

<p><b>Content:</b>
<ul class="bullet">
  <li>Encoder-Decoder Models</li>
  <li>Conditioned Generation and Search</li>
  <li>Ensembling</li>
  <li>Evaluation</li>
  <li>Types of Data to Condition On</li>
</ul>
</p>
<p>
<b>Reading Material</b>
<ul class="default">
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/pdf/1703.01619.pdf">Neural Machine Translation and Sequence-to-Sequence Models</a> Chapter 7</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D13/D13-1176.pdf">Recurrent Neural Translation Models</a> (Kalchbrenner and Blunsom 2013)</li>
  <li><i>Reference:</i> <a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">LSTM Encoder-Decoders</a> (Sutskever et al. 2014)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1908.10090">On NMT Search Errors and Model Errors: Cat Got Your Tongue?</a> (Stahlberg and Byrne. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1904.09751">The Curious Case of Neural Text Degeneration (nucleus / top-p sampling)</a> (Holtzman et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/P18-1082/">Hierarchical Neural Story Generation (top-k sampling)</a> (Fan et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2210.15191">Truncation Sampling as Language Model Desmoothing</a> (Hewitt et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2205.00978">Quality Aware Decoding</a> (Fernandes et al. 2022)</li>
  <li><i>Tool:</i> <a href="https://github.com/deep-spin/qaware-decode">Quality Aware Decoding</a></li>
  <li><i>Reference:</i> <a href="https://ufal.mff.cuni.cz/pbml/108/art-bahar-alkhouli-peter-brix-ney.pdf">Parameter Averaging</a> (Bahar et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D16/D16-1139.pdf">Knowledge Distillation</a> (Kim et al. 2016)</li>
  <li><i>Link:</i> <a href="http://statmt.org/wmt21/">WMT Shared Tasks</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2001.09977">Meena Chatbot</a> (Adiwardana et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">Generation from Images</a> (Karpathy and Li 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D15/D15-1199.pdf">Generation from Structured Data</a> (Wen et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1239.pdf">Challenges in Data-to-document Generation</a> (Wisemen et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/N16-1005/">Generation from Input+Tags</a> (Sennrich et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/N/N16/N16-1149.pdf">Generation from TED Talk Metadata</a> (Hoang et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://www.statmt.org/wmt20/">WMT Transalation Tasks</a></li>
  <li><i>Reference:</i> <a href="https://genie.apps.allenai.org/">GENIE Leaderboard</a></li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P02/P02-1040.pdf">BLEU</a> (Papineni et al. 2002)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1904.09675">BertScore</a> (Zhang et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.704.pdf">BLEURT</a> (Sellam et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.213.pdf">COMET</a> (Rei et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.8.pdf">PRISM</a> (Thompson and Post 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2106.11520">BARTScore</a> (Yuan et al. 2021)</li>
  <li><i>Reference:</i> <a href="http://www.statmt.org/wmt20/pdf/2020.wmt-1.77.pdf">WMT Metrics Shared Task</a> (Mathur et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.emnlp-main.751.pdf">Re-evaluating Evaluation in Text Summarization</a> (Bhandari et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2203.05482">Model Soups: Averaging Weights of Multiple Fine-Tuned Models Improves Accuracy Without Increasing Inference Time</a> (Wortsman et al. 2022)</li>
</ul>
</p>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-05-condlm.pdf">Conditioned LM Slides</a> <br/>
<b>Sample Code:</b> <a href="https://github.com/neubig/nn4nlp-code/tree/master/08-condlm">Conditioned LM Code Examples</a><br/>
</p>

