---
layout: post
title:  "Intro 3 - Language Modeling and NN Basics (9/5/2023)"
date:   2023-09-05 13:30:00 -0400
categories: classes
---

<p><b>Content:</b>
<ul class="bullet">
  <li>Language Modeling Problem Definition</li>
  <li>Count-based Language Models</li>
  <li>Measuring Language Model Performance: Accuracy, Likelihood, and Perplexity</li>
  <li>Log-linear Language Models</li>
  <li>Neural Network Basics</li>
  <li>Feed-forward Neural Network Language Models</li>
</ul>
</p>

<p>
<b>Reading Material</b>
<ul class="default">
  <li><i>Highly Recommended Reading:</i> <a href="https://link.springer.com/book/10.1007/978-3-031-02165-7">Goldberg Book</a> Chapter 8-9. </li>
  <li><i>Reference:</i> <a href="https://dash.harvard.edu/bitstream/handle/1/25104739/tr-10-98.pdf">An Empirical Study of Smoothing Techniques for Language Modeling</a> (Goodman 1998)</li>
  <li><i>Software:</i> <a href="https://github.com/kpu/kenlm">kenlm</a></li>
  <li><i>Reference:</i> <a href="http://repository.cmu.edu/cgi/viewcontent.cgi?article=2337&context=compsci">Maximum entropy (log-linear) language models</a>. (Rosenfeld 1996)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1608.05859.pdf">Using the Output Embedding</a>. (Press and Wolf 2016)</li>
  <li><i>Reference:</i> <a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">A Neural Probabilistic Language Model</a>. (Bengio et al. 2003, JMLR)</li>
</ul>
</p>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-03-lm.pdf">LM Slides</a> <br/>
<b>Sample Code:</b> <a href="https://github.com/neubig/anlp-code/tree/master/03-lm">LM Code Examples</a><br/>
</p>
