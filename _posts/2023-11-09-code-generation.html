---
layout: post
title:  "Guest Lecture by Zora Wang and Nikitha Rao - Code Generation (11/9/2023)"
date:   2023-11-9 13:30:00 -0400
categories: classes
---

<ul class="bullet">
  <li>Lexical-based evaluation</li>
  <li>Domain divergence</li>
  <li>Test creation</li>
  <li>Functional complexity</li>
  <li>Aligning code models</li>
</ul>
<ul class="default">
  <li><i>Highly Recommended Reading:</i> <a href="https://arxiv.org/abs/2107.03374">Evaluating Large Language Models Trained on Code</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2202.13169v3">A Systematic Evaluation of Large Language Models of Code</a></li>
  <li><i>Recommended Reading:</i> <a href="https://proceedings.neurips.cc/paper/2021/file/c2937f3a1b3a177d2408574da0245a19-Paper.pdf">PLUR: A unifying, graph-based view of program learning, understanding, and repair</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2308.12950v2">Code Llama: Open Foundation Models for Code</a></li>
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/abs/2305.06161">StarCoder: may the source be with you!</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2306.08568v1">WizardCoder: Empowering Code Large Language Models with Evol-Instruct</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2207.01780v3">CodeRL: Mastering Code Generation through Pretrained Models and Deep Reinforcement Learning</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2301.13816v4">Execution-based Code Generation using Deep Reinforcement Learning</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2308.10335v4">Can ChatGPT replace StackOverflow? A Study on Robustness and Reliability of Large Language Model Code Generation</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2307.10793v1">Addressing Compiler Errors: Stack Overflow or Large Language Models?</a></li>
  <li><i>Recommended Reading:</i> <a href="http://arxiv.org/abs/2305.01210v3">Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2102.04664v2">CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/1805.08949v1">Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2203.08388v2">MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2002.08155">CodeBERT: A Pre-Trained Model for Programming and Natural Languages</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2009.10297v2">CodeBLEU: a Method for Automatic Evaluation of Code Synthesis</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2105.09938v3">Measuring Coding Challenge Competence With APPS</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2108.07732">Program Synthesis with Large Language Models</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2211.11501v1">DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2212.09248v1">Natural Language to Code Generation in Interactive Data Science Notebooks</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2212.10481">Execution-Based Evaluation for Open-Domain Code Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2308.01861v2">ClassEval: A Manually-Crafted Benchmark for Evaluating LLMs on Class-level Code Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2303.12570v3">RepoCoder: Repository-Level Code Completion Through Iterative Retrieval and Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2310.06770v1">SWE-bench: Can Language Models Resolve Real-World GitHub Issues?</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2305.07922">CodeT5+: Open Code Large Language Models for Code Understanding and Generation</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2308.07124v1">OctoPack: Instruction Tuning Code Large Language Models</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2207.10397">CodeT: Code Generation with Generated Tests</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2307.04349v1">RLTF: Reinforcement Learning from Unit Test Feedback</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2304.05128">Teaching Large Language Models to Self-Debug</a></li>
  <li><i>Reference:</i> <a href="http://arxiv.org/abs/2302.08468v3">LEVER: Learning to Verify Language-to-Code Generation with Execution</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2310.01602">CAT-LM Training Language Models on Aligned Code And Tests</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a></li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a></li>
</ul>

<p>
<!--
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-15-dialog.pdf">Dialog Slides</a> <br/>
-->
</p>

