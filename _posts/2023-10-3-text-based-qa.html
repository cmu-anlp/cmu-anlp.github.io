---
layout: post
title:  "Applications - Text-based QA (10/3/2023)"
date:   2023-10-3 13:30:00 -0400
categories: classes
---

<ul class="bullet">
  <li>Machine Reading Datasets</li>
  <li>Methods for Encoding Context</li>
  <li>Methods for Multi-hop Reasoning</li>
  <li>Cavevats about Datasets</li>
</ul>
</p>
<ul class="default">
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D/D13/D13-1020.pdf">MCTest</a> (Richardson et al. 2013)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1083.pdf">RACE</a> (Lai et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D/D16/D16-1264.pdf">SQuAD</a> (Rajpurkar et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1147.pdf">TriviaQA</a> (Joshi et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P16-1223">A Thorough Examination of the CNN/Daily Mail Task</a> (Chen et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/D/D17/D17-1214.pdf">Adversarial Examples in SQuAD</a> (Jia and Liang 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1808.05326.pdf">Adversarial Construction of Datasets</a> (Zellers et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://storage.googleapis.com/pub-tools-public-publication-data/pdf/b8c26e4347adc3453c15d96a09e6f7f102293f71.pdf">Natural Questions</a> (Kwiatkowski et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1506.03340.pdf">Teaching Machines to Read and Comprehend</a> (Hermann et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P/P16/P16-1086.pdf">Attention Sum</a> (Kadlec et al. 2016)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1055.pdf">Attention over Attention</a> (Cui et al. 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1611.01603.pdf">Bidirectional Attention Flow</a> (Seo et al. 2017)</li>
  <li><i>Reference:</i> <a href="">UnifiedQA</a> (Khashabi et al. 2000)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/Q18-1023/">NarrativeQA</a> (Kočiský et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1809.09600.pdf">HotpotQA</a> (Yang et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1904.12106">Dataset Bias in Multi-hop Reasoning</a> (Chen and Durrett 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1410.3916.pdf">Memory Networks</a> (Weston et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1503.08895.pdf">End-to-end Memory Networks</a> (Sukhbaatar et al. 2015)</li>
  <li><i>Reference:</i> <a href="http://proceedings.mlr.press/v48/kumar16.pdf">Dynamic Memory Networks</a> (Kumar et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1609.05284.pdf">Learning to Stop Reading</a> (Shen et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1020.pdf">Coarse-to-fine Question Answering</a> (Choi et al. 2017)</li>
  <li><i>Reference:</i> <a href="http://aclweb.org/anthology/P/P17/P17-1171.pdf">Reading Wikipedia to Answer Open-Domain Questions</a> (Chen et al. 2017)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2001.11770">BREAK Dataset</a> (Wolfson et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1906.02916">Question Decomposition</a> (Min et al. 2019)</a>
  <li><i>Reference:</i> <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">Retrieval-augmented Generation</a> (Lewis et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2009.12756">Multi-hop Dense Retrieval</a> (Xiong et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1909.01066">Language Models as Knowledge Bases</a> (Petroni et al. 2019)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2002.08909.pdf">Retrieval-based Language Model Pre-training</a> (Guu et al. 2020)</a>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P19-1329.pdf">Exploring Numeracy in Embeddings</a> (Naik et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/D19-1534.pdf">Do NLP Models Know Numbers</a> (Wallace et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1903.00161">DROP Dataset</a> (Dua et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1912.04971.pdf">Neural Module Networsk for Reasoning Over Text</a> (Gupta et al. 2020)</li>
  <li><i>Reference:</i> <a href="http://papers.nips.cc/paper/6969-end-to-end-differentiable-proving.pdf">End-to-end Differentiable Proving</a> (Rocktäschel and Riedel 2017)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1808.07036">Question Answering in Context</a> (Choi et al. 2018)</a>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1808.07042.pdf">Contextual Question Answering</a> (Reddy et al. 2018)</a>
</ul>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-11-textbasedqa.pdf">Text-based QA Slides</a> <br/>
</p>

