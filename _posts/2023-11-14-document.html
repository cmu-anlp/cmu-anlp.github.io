---
layout: post
title:  "Learning 1 - Modeling Long Sequences (11/14/2023)"
date:   2023-11-14 13:30:00 -0400
categories: classes
---

<ul class="bullet">
</ul>
</p>
<ul class="default">
  <li><i>Reference:</i> <a href="https://www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf">RNN Language Models</a> (Mikolov et al 2010)</li>
  <li><i>Reference:</i> <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/rnn_ctxt.pdf">Larger Context RNNLMs</a> (Mikolov and Zweig 2012)</li>
  <li><i>Reference:</i> <a href="https://aclweb.org/anthology/P18-1117">Self Attention over Previous Sentence</a> (Voita et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1901.02860">Self Attention over Previous Vectors</a> (Dai et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1911.05507.pdf">Compressive Transformer</a> (Lillicrap et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1904.10509.pdf">Sparse Transformers</a> (Child et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2004.05150v2.pdf">Longformer: The Long-Document Transformer</a> (Beltagy et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2310.06825">Mistral-7B</a> (Jiang et al. 2023)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1905.07799.pdf">Adaptive Span Transformer</a> (Sukhbaatar et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1909.00015.pdf">Adaptively Sparse Transformers</a> (Correia et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2001.04451.pdf">Reformer</a> (Kitaev et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2006.04768.pdf">Linformer</a> (Wang et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2102.03902.pdf">Nystromformer</a> (Xiong et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2108.12409.pdf">ALiBi</a> (Press et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2205.09921.pdf">KERPLE</a> (Chi and Fan et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a> (Dao et al. 2022)</li>
  <li><i>Reference:</i> <a href="http://www.anthology.aclweb.org/J/J08/J08-1001.pdf">Evaluation: Sentence Scrambling</a> (Barzilay and Lapata 2008)</li>
  <li><i>Reference:</i> <a href="http://www.aclweb.org/anthology/N16-1098">Evaluation: Final Sentence Prediction</a> (Mostafazadeh et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/P/P16/P16-1144.pdf">Evaluation: Final Word Prediction</a> (Paperno et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2011.04006.pdf">Long Range Arena</a> (Tay et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://www.harmdevries.com/post/context-length/">In the long (context) run</a> (De Vries 2023)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2310.10638">In-Context Pretraining: Language Modeling Beyond Document Boundaries</a> (Shi et al. 2023)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/2111.00396.pdf">S4: Efficiently Modeling Long Sequences with Structured States Spaces</a> (Gu et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://openreview.net/pdf?id=qNLe3iq2El">MEGA: Moving Average Equipped Gated Attention</a> (Ma and Zhou et al. 2023)</li>
</ul>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-19-document.pdf">Long Sequences Slides</a> <br/>
</p>


