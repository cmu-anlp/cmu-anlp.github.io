---
layout: post
title:  "Representation 1 - Pre-training Methods (9/19/2023)"
date:   2023-09-19 13:30:00 -0400
categories: classes
---

<p><b>Content:</b>
<ul class="bullet">
  <li>Simple overview of multi-task learning</li>
  <li>Sentence embeddings</li>
  <li>BERT and variants</li>
  <li>Other language modeling objectives</li>
</ul>
</p>
<p>
<b>Reading Material</b>
<ul class="default">
  <li><i>Highly Recommended Reading:</i> <a href="http://jalammar.github.io/illustrated-bert/">Illustrated BERT</a> (Alammar 2019)</li>
  <li><i>Reference:</i> <a href="https://papers.nips.cc/paper/2015/file/7137debd45ae4d0ab9aa953017286b20-Paper.pdf">Language Model Transfer</a> (Dai et al. 2015)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1802.05365">ELMo: Deep Contextualized Word Representations</a> (Peters et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1410/">Sentence BERT</a> (Reimers and Gurevych 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Bidirectional Transformers</a> (Devlin et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1907.11692.pdf">RoBERTa: Robustly Optimized BERT</a> (Liu et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/pdf/1906.08237.pdf">XLNet: Autoregressive Training w/ Permutation Objectives</a> (Yang et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://openreview.net/forum?id=r1xMH1BtvB">ELECTRA: Pre-training Text Encoders as Discriminators</a> (Clark et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2005.14165">GPT-3</a> (Brown et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2204.02311">PaLM</a> (Chowdhery et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2109.07437">Should we be Pre-training?</a> (Dery et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2205.14082">Automating Auxiliary Learning</a> (Dery et al. 2022)</li>
</ul>
</p>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-07-pretraining.pdf">Pre-training Slides</a> <br/>
<b>Sample Code:</b> <a href="https://github.com/neubig/nn4nlp-code/tree/master/07-sentrep">Pre-training Code Examples</a><br/>
</p>
