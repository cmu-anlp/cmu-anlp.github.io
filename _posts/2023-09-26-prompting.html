---
layout: post
title:  "Representation 3 - Prompting (9/26/2023)"
date:   2023-09-26 13:30:00 -0400
categories: classes
---

<h4>Prompting + Sequence-to-sequence Pre-training</h4>
<ul class="bullet">
  <li>LLMs</li>
  <li>Prompting Methods</li>
  <li>Instruction Tuning</li>
</ul>

<ul class="default">
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/abs/2107.13586">Pre-Train, Prompt, and Predict. A Systematic Survey of Prompting Methods in NLP</a> (Liu et al. 2021) </li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1910.10683">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</a> (Raffel et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2: Language Models are Unsupervised Multitask Learners</a> (Radford et al. 2019)</li>
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/abs/2005.14165">GPT-3: Language Models are Few-Shot Learners</a> (Brown et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2212.04037">Demystifying Prompts in Language Models via Perplexity Estimation</a> (Gonen et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2020.emnlp-main.346">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</a> (Shin et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2021.acl-long.353/">Prefix-Tuning: Optimizing Continuous Prompts for Generation</a> (Li and Liang 2021)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2021.emnlp-main.243/">The Power of Scale for Parameter-Efficient Prompt Tuning</a> (Lester et al. 2021)</li>
  <li><i>Reference:</i> <a href="http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf">Calibrate Before Use: Improving Few-Shot Performance of Language Models</a> (Zhao et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2022.emnlp-main.759">Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?</a> (Min et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2211.09110">HELM: Holistic Evaluation of Language Models</a> (Liang et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2201.11903">Chain-of-Thought Prompting</a> (Wei et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2205.11916">Large Language Models are Zero-Shot Reasoners</a> (Kojima et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2203.11171">Self-Consistency Improves Chain-of-Thought Reasoning in Language Models</a> (Wang et al. 2023)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2021.findings-emnlp.244">Adapting Language Models for Zero-shot Learning by Meta-Tuning on Dataset and Prompt Collections</a> (Sanh et al. 2021)</li>
  <li><i>Recommended Reading:</i> <a href="https://arxiv.org/abs/2110.08207">T0: Multitask Prompted Training Enables Zero-Shot Task Generalization</a> (Sanh et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2109.01652">FLAN: Finetuned Language Models are Zero-Shot Learners</a> (Wei et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2210.11416">Scaling Instruction-Finetuned Language Models</a> (Chung et al. 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2204.05832">Which Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?</a> (Wang et al. 2022)</li>
</ul>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-09-prompting.pdf">Prompting Slides</a> <br/>
</p>

