---
layout: post
title:  "Applications 2 - Bias and Fairness (10/25/2022)"
date:   2022-10-25 13:30:00 -0400
categories: classes
---
<ul class="bullet">
  <li>Computational Social Science</li>
  <li>Types of Bias in NLP Models</li>
  <li>How to Prevent Bias in NLP</li>
</ul>
</p>

<p><b>References</b></p>
<ul>
<li><i>Highly Recommended Reading:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.485/">Language (Technology) is Power: A Critical Survey of “Bias" in NLP</a> (Blodgett et al. 2020)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/2021.acl-long.149/">Survey of Race and Racism in NLP</a></li>
<li><i>Reference:</i> <a href="https://dl.acm.org/doi/pdf/10.1145/301353.301410">Ethics of Persuasive Technology</a> (Berdichevsky and Neuenschwander, 1999)</li>
<li><i>Reference:</i> <a href="https://www.aies-conference.com/2018/contents/papers/main/AIES_2018_paper_9.pdf">Measuring and Mitigating Unintended Bias in Text Classification</a> (Dixon et al. 2017)</li>
<li><i>Reference:</i> <a href="http://modeltheory.org/papers/2016counterfactuals.pdf">Counterfactual Thought</a> (Byrne 2016)</li>
<li><i>Reference:</i> <a href="https://onlinelibrary.wiley.com/doi/abs/10.1111/phpr.12691">Norms in Counterfactual Selection</a> (Fazelpour 2020)</li>
<li><i>Reference:</i> <a href="https://papers.nips.cc/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings</a> (Bolukbasi et al. 2016)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/1608.07187">Semantics derived automatically from language corpora contain human-like biases</a> (Caliskan et al. 2017)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/1903.10561">On Measuring Social Biases in Sentence Encoders</a> (May et al. 2019)</li>
<li><i>Reference:</i> <a href="https://www.pnas.org/content/117/14/7684">Racial disparities in automated speech recognition</a> (Koenecke et al. 2020)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/2020.acl-main.560/">State and Fate of Linguistic Diversity</a> (Joshi et al. 2020)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/2110.06733">Systematic Disparities in Language Technology Performance</a> (Blasi et al. 2021)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/N18-2002/">Gender Bias in Coreference Resolution</a> (Rudinger et al. 2018)</li>
<li><i>Reference:</i> <a href="http://proceedings.mlr.press/v28/zemel13.html">Learning Fair Representations</a> (Zemel et al. 2013)</li>
<li><i>Reference:</i> <a href="http://proceedings.mlr.press/v37/ganin15.pdf">Unsupervised Domain Adaptation by Backpropagation</a> (Ganin and Lempitsky 2015)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/N19-1061/">Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them</a> (Gonen and Goldberg 2019)</li>
<li><i>Reference:</i> <a href="https://openreview.net/forum?id=Sklgs0NFvr">Learning The Difference That Makes A Difference With Counterfactually-Augmented Data</a> (Kaushik et al. 2020)</li>
<li><i>Reference:</i> <a href="https://openreview.net/forum?id=HHiiQKWsOcV">Explaining the Efficacy of Counterfactually Augmented Data</a> (Kaushik et al. 2021)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.690/">Reducing Gender Bias in Neural Machine Translation as a Domain Adaptation Problem</a> (Saunders and Byrne 2020)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.291/">Towards Controllable Biases in Language Generation</a> (Sheng et al. 2020)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/W17-1601/">Gender as a Variable in Natural-Language Processing: Ethical Considerations</a> (Larson 2017)</li>
<li><i>Reference:</i> <a href="https://www.cc.gatech.edu/~beki/cs4001/Winner.pdf">Do Artifacts Have Politics?</a> (Winner 1980)</li>
<li><i>Reference:</i> <a href="https://www.youtube.com/watch?v=fMym_BKWQzk">The Trouble With Bias</a> (Crawford 2017)</li>
<li><i>Reference:</i> <a href="https://www.aclweb.org/anthology/2020.acl-main.468/">Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview</a> (Shah et al. 2020)</li>
<li><i>Reference:</i> <a href="https://www.sciencedirect.com/science/article/pii/S2666389921000611">Moving beyond “algorithmic bias is a data problem"</a> (Hooker 2021)</li>
<li><i>Reference:</i> <a href="https://fairmlbook.org">Fairness and Machine Learning</a> (Barocas et al. 2019)</li>
<li><i>Reference:</i> <a href="https://cacm.acm.org/magazines/2018/3/225484-computational-social-science-computer-science-social-data/fulltext">Computational Social Science ≠ Computer Science + Social Data</a> (Wallach 2018)</li>
<li><i>Reference:</i> <a href="https://electionemails2020.org/assets/manipulative-political-emails-working-paper.pdf">Manipulative Tactics in Political Emails from the 2020 U.S. Election</a> (Mathur et al. 2020)</li>
<li><i>Reference:</i> <a href="http://www.collingwoodresearch.com/uploads/8/3/6/0/8360930/grimmer_and_stewart_2012.pdf">Text as Data</a> (Grimmer and Stewart 2012)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/D18-1393.pdf">Agenda Setting in Russian News</a> (Field et al. 2018)</li>
<li><i>Reference:</i> <a href="https://aclanthology.org/P16-1141/">Diachronic Word Embeddings</a> (Hamilton et al. 2016)</li>
<li><i>Reference:</i> <a href="https://arxiv.org/abs/1711.08412">Word Embeddings Quantify 100 Years of Gender and Ethnic Stereotypes</a> (Garg et al. 2017)</li>
</ul>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-14-bias.pdf">Bias/Fairness Slides</a> <br/>
</p>

