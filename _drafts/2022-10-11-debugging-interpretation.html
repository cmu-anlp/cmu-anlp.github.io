---
layout: post
title:  "Experimentation 2 - Interpreting and Debugging NLP Models (10/11/2022)"
date:   2022-10-11 13:30:00 -0400
categories: classes
---

<p><b>Content</b></p>
<ul class="bullet">
  <li>Neural NLP model debugging methods</li>
  <li>Probing, attribution techniques, and interpretable evaluation</li>
</ul>
</p>
<ul class="default">
  <li><i>Recommended Reference:</i> <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a> (Molnar 2022)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1910.10683">T5: Larger Models are Better</a> (Raffel et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2001.08361">Scaling Laws for Neural Language Models</a> (Kaplan et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/2002.11794">Train Large, then Distill</a> (Li et al. 2020)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/N19-4007/">compare-mt</a> (Neubig et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2021.acl-demo.34/">ExplainaBoard</a> (Liu et al. 2021)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1602.04938">Local Perterbations</a> (Ribeiro et al. 2016)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1711.06104">Gradient-based Explanations</a> (Ancona et al. 2018)</li>
  <li><i>Reference:</i> <a href="https://arxiv.org/abs/1905.06316">Edge Probing</a> (Tenney et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/D19-1275/">Control Tasks for Probing</a> (Hewitt et al. 2019)</li>
  <li><i>Reference:</i> <a href="https://aclanthology.org/2020.emnlp-main.14/">Information Theoretic Probing</a> (Voita et al. 2020)</li>
</ul>

<p>
<b>Slides:</b> <a href="{{ site.baseurl }}/assets/slides/anlp-12-interpretation.pdf">Interpretation Slides</a> <br/>
</p>
